{% extends 'base.html.twig' %}
{% block body %}
    <div class="container">
        <div class="row">
            <div class="col-sm-3">
                <div class="sidebar-nav">
                    <div class="navbar navbar-default" role="navigation">
                        <div class="navbar-header">
                            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".sidebar-navbar-collapse">
                                <span class="sr-only">Toggle navigation</span>
                                <span class="icon-bar"></span>
                                <span class="icon-bar"></span>
                                <span class="icon-bar"></span>
                            </button>
                            <span class="visible-xs navbar-brand">Sidebar menu</span>
                        </div>
                        <div class="navbar-collapse collapse sidebar-navbar-collapse">
                            <ul class="nav navbar-nav">
                                <li class="active"><a href="#">Parsing DBpedia Data</a></li>
                                <li><a href="#">Network Analysis</a></li>
                                <li><a href="#">Visualization</a></li>
                                <li><a href="#">UI framework</a></li>

                            </ul>
                        </div><!--/.nav-collapse -->
                    </div>
                </div>
            </div>
            <div id="wikiContent" class="col-sm-9">
                <div id="wikiPageHeader">
                    <p>Politician Map in English Wikipedia</p>
                    <p>[ Parsing Wikipedia ]</p>
                </div>
                <div id="wikiPageConent">
                    <p>Parsing Wikipedia articles was a part of a Research Project of creation a Politician Map in English Wikipedia. List of articles was delivered by Marcel (who parsed DBpedia).
                    </p>
                    <p>The main goal of the project is to create a web platform that illustrates a map (graph) of politicians and interconnections between them with applied filters as sex, nationality, etc.
                    </p>
                    <p>The main goal of parsing Wikipedia team is to retrieve internal links between articles about politicians from 2001 to 2016 with a month as a step. And then create a set of edge lists with the information of connected politicians (“connection” here is an outgoing link=link from one politician’s article to another’s).</p>
                    <h3>Constraints and issues during the parsing Wikipedia</h3>
                    <ul class="custom-bullet">
                        <li>Memory leakage - after first attempt of running our script we realized that there is some memory leak and our script eats up 2gb of ram after parsing ~3500 articles. We did some code profiling and determined that the problem was in one of the external libraries. To work around this problem we rewrote our code to parse our article list in chunks of 1000 articles and called it from bash script in a loop.
                        </li>
                        <li>Python libraries – some of the libraries were responsible for the memory leakage described above (but we didn’t find out which one)
                        </li>
                        <li>Time – all parsing took 4 days via virtual servers
                        </li>
                    </ul>
                    <h3>Process</h3>
                    <p>We created</p>
                    <p>“Politician_data_tracker.csv”- for a list of politicians (wiki address and ID) same as a list from DBpedia</p>
                    <p>“parsed_data.csv” - for a list of politicians after it’s parsed. So the code won’t parse parsed data again. </p>
                    <p>“profile-data” folder – to store final deliverables</p>
                    <p>Except standard python libraries we used such libraries as mwclient, wikitextparser and pickle.</p>
                    <ul class="custom-bullet">
                        <li>With mwclient we started retrieving of revisions of articles metadata (Revision ID and Timestamp).
                        </li>
                        <li>Then we filtered retrieved data to leave one Revision per month. And retrieved wikitext of given revisions.
                        </li>
                        <li>Then with wikitextparser we parsed wikitext and extracted internal Wikipedia links</li>
                        <li>With python internal library called pickle we stored extracted links with info about month and year as python dictionaries in the “profile-data” folder and added new entries into files “Politician_data_tracker.csv” and “parsed_data.csv”. Note: files “Politician_data_tracker.csv” and “parsed_data.csv” were dynamically updating during the parsing.</li>
                        <li>Separate script was used to create edge lists.</li>
                    </ul>



                </div>
            </div>
        </div>
    </div>
{% endblock %}